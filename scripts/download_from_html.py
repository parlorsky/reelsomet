#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–∏—Ç HTML –∏–∑ txt —Ñ–∞–π–ª–∞, —Å–∫–∞—á–∏–≤–∞–µ—Ç –≤–∏–¥–µ–æ, —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ CSV

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
- –í–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–∫—Å–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö (rate limit)
- Retry –ø—Ä–æ–≤–∞–ª–µ–Ω–Ω—ã—Ö URL –≤ –∫–æ–Ω—Ü–µ

Usage:
  python download_from_html.py page.txt
  python download_from_html.py page.txt -o ./videos
  python download_from_html.py page.txt --transcribe
"""

import re
import sys
import os
import json
import asyncio
import argparse
import csv
import subprocess
import tempfile
from datetime import datetime
from pathlib import Path

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
try:
    from rename_by_popularity import parse_engagement, rename_files as rank_files
    HAS_RANKING = True
except ImportError:
    HAS_RANKING = False

# –ó–∞–≥—Ä—É–∂–∞–µ–º .env
try:
    from dotenv import load_dotenv
    env_path = Path(__file__).resolve().parent.parent / '.env'
    load_dotenv(env_path)
except ImportError:
    pass

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º downloader
try:
    from instagram_downloader import SnapInstaDownloader, ProxyManager, HAS_SOCKS
except ImportError:
    print("–û—à–∏–±–∫–∞: –Ω–µ –Ω–∞–π–¥–µ–Ω instagram_downloader.py")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ")
    sys.exit(1)

# OpenAI –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False


def extract_instagram_urls(html: str) -> list:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ—Å—Ç—ã/reels –∏–∑ HTML"""
    pattern = r'href="(/[^/]+/(?:reel|p)/[A-Za-z0-9_-]+/[^"]*)"'
    matches = re.findall(pattern, html)

    seen = set()
    unique = []
    for path in matches:
        clean_path = path.split('?')[0]
        if clean_path not in seen:
            seen.add(clean_path)
            unique.append(f"https://www.instagram.com{clean_path}")

    return unique


def extract_audio(video_path: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ –≤ mp3"""
    audio_path = tempfile.mktemp(suffix='.mp3')
    try:
        subprocess.run([
            'ffmpeg', '-i', video_path,
            '-vn', '-acodec', 'libmp3lame', '-q:a', '2',
            '-ar', '16000', '-ac', '1',
            '-y', audio_path
        ], capture_output=True, check=True, timeout=120)
        return audio_path
    except (subprocess.CalledProcessError, subprocess.TimeoutExpired):
        return None


def transcribe_audio(audio_path: str, client: OpenAI) -> str:
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ OpenAI API"""
    try:
        with open(audio_path, 'rb') as audio_file:
            transcription = client.audio.transcriptions.create(
                model="gpt-4o-transcribe",
                file=audio_file,
                response_format="text"
            )
        return transcription if isinstance(transcription, str) else transcription.text
    except Exception as e:
        return f"[–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: {str(e)[:100]}]"


def transcribe_video(video_path: str, client: OpenAI) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ"""
    audio_path = extract_audio(video_path)
    if not audio_path:
        return "[–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—É–¥–∏–æ]"

    try:
        result = transcribe_audio(audio_path, client)
        return result
    finally:
        if os.path.exists(audio_path):
            os.remove(audio_path)


async def scrape_metadata(page, url: str) -> dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç–∞ –∏–∑ Instagram —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    metadata = {
        'url': url,
        'shortcode': url.rstrip('/').split('/')[-1],
        'description': '',
        'date': '',
        'author': '',
    }

    try:
        await page.goto(url, wait_until='domcontentloaded', timeout=30000)
        await page.wait_for_timeout(2000)

        scripts = await page.query_selector_all('script[type="application/ld+json"]')
        for script in scripts:
            try:
                text = await script.inner_text()
                data = json.loads(text)
                if isinstance(data, dict):
                    if 'articleBody' in data:
                        metadata['description'] = data.get('articleBody', '')[:500]
                    elif 'caption' in data:
                        metadata['description'] = data.get('caption', '')[:500]
                    if 'dateCreated' in data:
                        metadata['date'] = data.get('dateCreated', '')[:10]
                    elif 'uploadDate' in data:
                        metadata['date'] = data.get('uploadDate', '')[:10]
                    if 'author' in data:
                        author = data.get('author', {})
                        if isinstance(author, dict):
                            metadata['author'] = author.get('name', '') or author.get('identifier', '')
                        elif isinstance(author, str):
                            metadata['author'] = author
            except:
                pass

        if not metadata['description']:
            try:
                og_desc = await page.query_selector('meta[property="og:description"]')
                if og_desc:
                    content = await og_desc.get_attribute('content')
                    if content:
                        metadata['description'] = content[:500]
            except:
                pass

        if not metadata['description']:
            try:
                title = await page.title()
                if title and 'Instagram' in title:
                    metadata['description'] = title[:500]
            except:
                pass

    except Exception as e:
        pass

    return metadata


async def setup_proxy_manager():
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç ProxyManager"""
    if not HAS_SOCKS:
        return None

    print("\nüîÑ –ó–∞–≥—Ä—É–∂–∞—é SOCKS5 –ø—Ä–æ–∫—Å–∏...")
    proxy_manager = ProxyManager()
    await proxy_manager.fetch_proxies()
    await proxy_manager.test_proxies(limit=20)

    if not proxy_manager.working_proxies:
        print("‚ö†Ô∏è  –†–∞–±–æ—á–∏–µ –ø—Ä–æ–∫—Å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return None

    print(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(proxy_manager.working_proxies)} —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏")
    return proxy_manager


async def download_batch(urls: list, output_dir: str, max_tabs: int, proxy_manager=None) -> list:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –ø–∞–∫–µ—Ç URL"""
    async with SnapInstaDownloader(headless=True, max_tabs=max_tabs, proxy_manager=proxy_manager) as dl:
        return await dl.download_all(urls, output_dir)


async def main_async(html_file: str, output_dir: str, use_proxy: bool, max_tabs: int, do_transcribe: bool, do_rank: bool = False):
    # –ß–∏—Ç–∞–µ–º HTML –∏–∑ —Ñ–∞–π–ª–∞
    print(f"üìÑ –ß–∏—Ç–∞—é —Ñ–∞–π–ª: {html_file}")
    with open(html_file, 'r', encoding='utf-8') as f:
        html = f.read()

    # –ü–∞—Ä—Å–∏–º —Å—Å—ã–ª–∫–∏
    urls = extract_instagram_urls(html)

    if not urls:
        print("‚ùå –°—Å—ã–ª–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ñ–∞–π–ª–µ!")
        return False

    print(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫:")
    for i, url in enumerate(urls, 1):
        shortcode = url.rstrip('/').split('/')[-1]
        print(f"  {i}. {shortcode}")

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É
    os.makedirs(output_dir, exist_ok=True)

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    proxy_manager = None
    if use_proxy:
        proxy_manager = await setup_proxy_manager()

    # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    print(f"\nüìù –°–æ–±–∏—Ä–∞—é –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ...")
    from playwright.async_api import async_playwright

    all_metadata = {}
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        page = await context.new_page()

        for i, url in enumerate(urls, 1):
            shortcode = url.rstrip('/').split('/')[-1]
            print(f"  [{i}/{len(urls)}] {shortcode}...", end=' ', flush=True)
            meta = await scrape_metadata(page, url)
            all_metadata[url] = meta
            print(f"‚úì" if meta['description'] else "–Ω–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è")

        await browser.close()

    # === –ü–ï–†–í–´–ô –ü–†–û–•–û–î: —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ ===
    print(f"\n‚¨áÔ∏è  –°–∫–∞—á–∏–≤–∞—é {len(urls)} –≤–∏–¥–µ–æ...")
    print(f"   –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫: {max_tabs}")

    results = await download_batch(urls, output_dir, max_tabs, proxy_manager)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ–ª—å–∫–æ –æ—à–∏–±–æ–∫
    failed = [r for r in results if not r['success']]
    success_count = len(results) - len(failed)

    print(f"\nüìä –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥: {success_count}/{len(urls)} —Å–∫–∞—á–∞–Ω–æ")

    # === –ê–í–¢–û-RETRY —Å –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ ===
    if len(failed) > 0:
        failed_urls = [r['url'] for r in failed]

        # –ï—Å–ª–∏ –Ω–µ –±—ã–ª–æ –ø—Ä–æ–∫—Å–∏ –∏ >20% –æ—à–∏–±–æ–∫ ‚Äî –≤–∫–ª—é—á–∞–µ–º –ø—Ä–æ–∫—Å–∏
        if not proxy_manager and len(failed) > len(urls) * 0.2:
            print(f"\n‚ö†Ô∏è  {len(failed)} –æ—à–∏–±–æ–∫ ({len(failed)*100//len(urls)}%) ‚Äî –≤–∫–ª—é—á–∞—é –ø—Ä–æ–∫—Å–∏...")
            proxy_manager = await setup_proxy_manager()

        if proxy_manager or len(failed) <= 5:
            # Retry –ø—Ä–æ–≤–∞–ª–µ–Ω–Ω—ã—Ö
            max_retries = 3
            for retry_num in range(max_retries):
                if not failed_urls:
                    break

                print(f"\nüîÑ Retry #{retry_num + 1}: {len(failed_urls)} URL...")

                retry_results = await download_batch(failed_urls, output_dir, max(1, max_tabs // 2), proxy_manager)

                # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                still_failed = []
                for rr in retry_results:
                    # –ù–∞—Ö–æ–¥–∏–º –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
                    for i, r in enumerate(results):
                        if r['url'] == rr['url']:
                            results[i] = rr
                            break

                    if not rr['success']:
                        still_failed.append(rr['url'])

                retry_success = len(failed_urls) - len(still_failed)
                print(f"   ‚úì –°–∫–∞—á–∞–Ω–æ: {retry_success}/{len(failed_urls)}")

                failed_urls = still_failed

                if not failed_urls:
                    break

                # –ñ–¥—ë–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º retry
                if retry_num < max_retries - 1 and failed_urls:
                    print(f"   ‚è≥ –ñ–¥—É 10 —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    await asyncio.sleep(10)

    # === –¢–†–ê–ù–°–ö–†–ò–ë–ê–¶–ò–Ø ===
    transcriptions = {}
    if do_transcribe:
        if not HAS_OPENAI:
            print("\n‚ö†Ô∏è  OpenAI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. pip install openai")
        elif not os.environ.get('OPENAI_API_KEY'):
            print("\n‚ö†Ô∏è  OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω")
        else:
            # –°–æ–±–∏—Ä–∞–µ–º —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã–µ
            to_transcribe = []
            for r in results:
                if r['success'] and r.get('files'):
                    video_path = r['files'][0]
                    if os.path.exists(video_path):
                        to_transcribe.append((r['url'], video_path))

            if to_transcribe:
                print(f"\nüé§ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é {len(to_transcribe)} –≤–∏–¥–µ–æ...")
                client = OpenAI()

                from concurrent.futures import ThreadPoolExecutor, as_completed

                def transcribe_task(item):
                    url, video_path = item
                    shortcode = url.rstrip('/').split('/')[-1]
                    text = transcribe_video(video_path, client)
                    return url, shortcode, text

                with ThreadPoolExecutor(max_workers=5) as executor:
                    futures = {executor.submit(transcribe_task, item): item for item in to_transcribe}
                    done = 0
                    for future in as_completed(futures):
                        done += 1
                        url, shortcode, text = future.result()
                        transcriptions[url] = text
                        print(f"  [{done}/{len(to_transcribe)}] {shortcode}: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

    # === –°–û–•–†–ê–ù–Ø–ï–ú CSV ===
    final_data = []
    for r in results:
        url = r['url']
        meta = all_metadata.get(url, {})
        final_data.append({
            'url': url,
            'shortcode': meta.get('shortcode', url.rstrip('/').split('/')[-1]),
            'description': meta.get('description', ''),
            'date': meta.get('date', ''),
            'author': meta.get('author', ''),
            'video_file': r.get('files', [''])[0] if r['success'] else '',
            'transcription': transcriptions.get(url, ''),
            'status': 'OK' if r['success'] else r.get('error', 'Error'),
        })

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    csv_file = os.path.join(output_dir, f"data_{timestamp}.csv")

    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['url', 'shortcode', 'description', 'date', 'author', 'video_file', 'transcription', 'status'])
        writer.writeheader()
        writer.writerows(final_data)

    # === –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï –ü–û –ü–û–ü–£–õ–Ø–†–ù–û–°–¢–ò ===
    if do_rank:
        if not HAS_RANKING:
            print("\n‚ö†Ô∏è  –ú–æ–¥—É–ª—å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω (rename_by_popularity.py)")
        else:
            print(f"\nüìä –†–∞–Ω–∂–∏—Ä—É—é —Ñ–∞–π–ª—ã –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏...")
            # –î–æ–±–∞–≤–ª—è–µ–º engagement –º–µ—Ç—Ä–∏–∫–∏
            for row in final_data:
                likes, comments = parse_engagement(row.get('description', ''))
                row['_likes'] = likes
                row['_comments'] = comments
                row['_engagement'] = likes + comments * 10

            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∏ –æ–±–Ω–æ–≤–ª—è–µ–º CSV
            ranked_data = rank_files(final_data, dry_run=False)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π CSV
            sorted_ranked = sorted(ranked_data, key=lambda x: x.get('_rank', 999))
            fieldnames = ['url', 'shortcode', 'description', 'date', 'author', 'video_file', 'transcription', 'status']
            clean_rows = [{k: row.get(k, '') for k in fieldnames} for row in sorted_ranked]

            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(clean_rows)

    # === –ò–¢–û–ì–ò ===
    success = sum(1 for r in results if r['success'])
    failed = sum(1 for r in results if not r['success'])
    transcribed = len(transcriptions)

    print(f"\n{'='*50}")
    print("üìä –ò–¢–û–ì–ò:")
    print(f"  ‚úì –°–∫–∞—á–∞–Ω–æ: {success}/{len(urls)}")
    print(f"  ‚úì –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–æ: {transcribed}")
    print(f"  üìÅ –ü–∞–ø–∫–∞: {output_dir}")
    print(f"  üìÑ CSV: {csv_file}")

    if failed > 0:
        print(f"\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å ({failed}):")
        for r in results:
            if not r['success']:
                shortcode = r['url'].rstrip('/').split('/')[-1]
                print(f"  - {shortcode}: {r.get('error', 'Unknown error')}")

    return success > 0


def main():
    parser = argparse.ArgumentParser(
        description='–°–∫–∞—á–∏–≤–∞–µ—Ç Instagram –≤–∏–¥–µ–æ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–µ–π',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:
  python download_from_html.py instagram.txt
  python download_from_html.py page.txt -o ./my_videos
  python download_from_html.py page.txt --transcribe

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
  - –í–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–∫—Å–∏ –ø—Ä–∏ >20% –æ—à–∏–±–æ–∫
  - Retry –ø—Ä–æ–≤–∞–ª–µ–Ω–Ω—ã—Ö URL (–¥–æ 3 —Ä–∞–∑)

–î–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: export OPENAI_API_KEY=sk-...
        """
    )
    parser.add_argument('html_file', help='–§–∞–π–ª —Å HTML –∫–æ–¥–æ–º Instagram')
    parser.add_argument('-o', '--output', default='../downloads', help='–ü–∞–ø–∫–∞ –¥–ª—è –≤–∏–¥–µ–æ')
    parser.add_argument('--proxy', action='store_true', help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏ —Å—Ä–∞–∑—É')
    parser.add_argument('-t', '--tabs', type=int, default=3, help='–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫')
    parser.add_argument('--transcribe', action='store_true', help='–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ OpenAI')
    parser.add_argument('--rank', action='store_true', help='–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ (001_xxx.mp4)')

    args = parser.parse_args()

    if not os.path.exists(args.html_file):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.html_file}")
        sys.exit(1)

    try:
        ok = asyncio.run(main_async(
            args.html_file,
            args.output,
            args.proxy,
            args.tabs,
            args.transcribe,
            args.rank
        ))
        sys.exit(0 if ok else 1)
    except KeyboardInterrupt:
        print("\n‚õî –û—Ç–º–µ–Ω–µ–Ω–æ")
        sys.exit(1)


if __name__ == '__main__':
    main()
